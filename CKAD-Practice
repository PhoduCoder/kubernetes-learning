CKAD-Practice

#Imperative command to run a pod
#Create a pod
kubectl run <name-of-pod> --image=nginx:latest

#Delete a pod without waiting
kubectl delete pod <pod-name> --force

#create pod and also expose a port
kubectl run <my-pod-name> --image=nginx --port 80 

#Create busybox pod with a command "echo hello world"
kubectl run mypod --image=busybox --command -- /bin/sh -c "echo hello world"

#Describe pods with various verbosity 
kubectl get pod mypod -v=7
# Helps you get other details of what's happening behind the scenes.
kubectl get pod mypod -v=8
kubectl get pod mypod -v=9

#Creation of pods with a given set of labels
kubectl run mypod --image=nginx -labels="env=test,app=myapp"

#To query the pods with the labels
#For a single label, -l works
kubectl get pods -l "app=myapp"

For multiple labels using -l option, use a ; in between
kubectl get pods -l app=myapp;env=test
kubectl get pods -l app=myapp,env=test 
Also works

use --selector also works 
kubectl get pods --selector "app=myapp,env=prod"

#Creation of multipod containers
#Create three busybox with various commands executed in each of them

#This will have to be created using a manifest
# We create the initial manifest using the --dry-run 
# and -o yaml option in our first command
# direct the output to a file and then
# edit the file contents to have more containers
# in it. 

#Keep in mind that --dry-run option has to be specified
# before the command option'

kubectl run multicont --dry-run -o yaml --command -- /bin/sh -c "ls; sleep 3600" > multi.yaml

Then add two more containers in the manifest

#If we wanted to check the utilization for the containers
kubectl top pods <pod-name> --containers

#Create a Pod with main container busybox and which executes
# this “while true; do echo ‘Hi I am from Main container’
# >> /var/log/index.html; sleep 5; done” and with sidecar 
#container with nginx image which exposes on port 80. 
#Use emptyDir Volume and mount this volume on path /var/log for
# busybox and on path /usr/share/nginx/html for nginx container.
# Verify both containers are running.

STEPS :
a) Create the pod template using the --dry-run command 
b) Add the other container alongwith the volume
c) Remember volume is to be added with the pod specs 
while volumeMount is to be added at containerspec

======

Remember that the shortcut to create a pod was 
kubectl run 
The only mandatory argument required is --image
However, you can supply a lot of other arguments such as 
--port , --command.
Also very handy is the --dry-run=client -o=yaml option that gives us
an ability to save the object into a YAML manifest 
without creating one

=======

Now let's move to Deployments 

The imperative command to create a deployment is 
kubectl create deploy <name-of-deploy> --image=nginx 

As like with pods, you can save the manifest in a YAML file
without creating the objects using the --dry-run=client
and -o=yaml and storing it into a YAML file

====

Note that if you were to create a busybox deployment and trying to 
execute some command, there is a slight change from pod, in that
you don't have to specify the --command explicitly, rather you just pass
a -- followed by command that you want to run. 

As an example, 
kubectl create deploy testdeploy --image=busybox --dryrun=client -o=yaml -- 
/bin/sh -c "echo Hello World!; sleep 3600;" >> my-test-deploy.yaml 

====

You can label the deployments similarly as to how 
you would label the pods or for that matter any other API resource

kubectl label deploy <name-of-deploy> {env=test,bu=bu1}

You can look for deploy with particular labels using the -l flag
As an example,
kubectl get deploy -l costcenter=3606,env=test,bu=dept1

Annotations Syntax wise work the same, but there isn't a matching selector 
that you can use to group Annotations wise, i.e. -l option 
will only apply for labels and not Annotations

====

If you want to scale a deployment, two ways to approach this
First is to add the replicas count in your manifest 
The second option is to use the imperative command of kubectl scale deploy

As an example, you will use something like below to scale the replica count to 4 for an nginx deployment

kubectl scale deploy <name-of-deploy> --replicas=4

====
Editing the image of a pod,deploy,daemonset, statefulset,cronjob imperatively

 kubectl set image (-f FILENAME | TYPE NAME) CONTAINER_NAME_1=CONTAINER_IMAGE_1 ... CONTAINER_NAME_N=CONTAINER_IMAGE_N
[options]

kubectl set image deploy/webserver nginx=busybox 

===

Kubectl rollout commands are useful to manage the rollouts of 
deployments
daemonsets
statefulsets

kubectl rollout status deploy/abc
get the status of rollout of deployments, daemonsets
and statefulsets

# Rollback to the previous deployment
  kubectl rollout undo deployment/abc

  kubectl rollout history deploy/abc
====
To autoscale 
kubectl autoscale deploy <name> --cpu-percent=40 --min=1 --max=10
===
Creating Jobs 

kubectl create job <name-of-job> --image=node --dry-run=client 
-o=yaml -- node -v >> job-manifest-sample.yaml

====
Creating cronjobs 

You only have to add a schedule in the cron format 

kubectl create cronjob my-job --image=busybox --schedule="*/1 * * * *" -- date

====

kubectl create job recurjob --image=busybox --dry-run=client -o=yaml -- date >> test-job.yam
l

Define parallelism to configure number of jobs in parallel
Define completions as say 'n' if you want to run n jobs one after the 
other in succession

====

Persistence 

PV is what to storage as Node is to compute
PVC is what ties a request for a storage to a PV
In your pod/deployment manifest, you should
specify PVC in the volume TYPE

The retain policy for PV can be 
either reclaim or delete. 

For dynamically provisioned PersistentVolumes, the default reclaim policy is "Delete"

To change from delete to reclaim , use
=====
kubectl patch pv <your-pv-name> -p '{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}'

=====

For reclaim, the PV is not deleted
nor the underlying storage asset.
The PV is although considered released
But this PV is not available for another reclaim

For delete, the PV alongwith the storage asset is
deleted. 
Volumes that were dynamically provisioned inherit 
the reclaim policy of their StorageClass, 
which defaults to Delete

Say when a pod is using a PVC 
and someone manually deletes the PVC or the PV
because of the K8s finalizers attached to these resources
these are not deleted or terminated immediately.

Once the pod with the PVC claim is deleted is 
when these PV and PVCs will be deleted. 

You can reserve a PV for a given PVC
by specifying that PVC claimRef in its manifest

===
Understanding Access Modes

====

ConfigMaps for unconfidential information
Eg. database host 
1 MiB limit 

CM has data or binarydata fields
instead of spec 

Both data and binarydata are optional

data fields => UTF-8 strings
binarydata => base64-enccoded string 

name of cm => valid DNS subdomain

Each key must be alphanumeric or contain 
.,_,-

The keys stored in data must not overlap with the
keys in the binaryData field.

you can add an immutable field to a ConfigMap definition 
to create an immutable ConfigMap.

There are various ways to consume a cm
using as env variables in your containers
or mounting them as volumes 

one of the advantages of using cm as mounted volumes 
instead of env variables 
is that with cm mounted as volumes
they are automatically updated for the containers , when the cm is updated
whereas for env variables they require 
a restart of the pods

The kubelet checks whether the mounted ConfigMap is fresh on every periodic sync.
even with volumes mounted configmap, there is some delay
which equals 
kubelet sync period + cache propagation delay

A container using a ConfigMap as a subPath volume mount will NOT receive ConfigMap updates.

by adding immutable: true 
prevents changes to cm. 

Once a cm is marked immutable 
it is not possible 
to change the contents of  cm 
and you must delete and create a new cm. 
=====

There are two imperative ways to create configmap
One is from-literal where you can pass 
key=value pairs from command line

Other is to use --from-file 

Examples

kubectl create cm testcm1 --from-literal=key1=value1
#In this case we can only have one key 

Say if we wanted to create more than one keys using 
this literal method, one hack is to do the following 

kubectl create cm testliteral --from-literal=key1=value1 \
--dry-run=client -o=yaml >> test-cm.yaml 

Then create one more key value in this test-cm.yaml

Other and easier way is to just repeat the same thing again
kubectl create cm cm-name --from-literal=key1=value1 --from-literal=key2=value2


TIP:
If you wanted to create a file from command line
using echo but wanted to have it contain newline symbols
You should use the echo with -e as an option 

Command:
echo -e "app=MyApp\nenv=prod\nauthor=grv" | tee -a ui.properties

#To create configmap from a given file
kubectl create cm file-cm --from-file=app.properties

#To create configmap from all files within a directory
kubectl create cm dir-cm --from-file=config/

=====
TIP: 
To refer to what all options are available for each apiresource
Use the 
kubectl explain command

Example:
kubectl explain pod.spec.containers
Will list out all the different specification for containers within a pod

====

Secrets

Much like ConfigMaps,secrets are also used to pass 
information to pods. 
However secrets are meant for sensitive data and 
contains data and stringdata instead of 
data and binarydata instead of spec 
like configmaps.

Much like configmap, secrets can also 
be mounted either as volumes 
or passed as environment variables.

Secrets like configmap also have a size limit of 1Mib

===
Image pulling secrets 
When kubelet downloads image 
from a private container registry, you can 
configure "image pull secrets" to authenticate the 
kubelet to a private registry from which the kubelet pulls 
the image from.

====
MANUAL WAY
Note that for this you will have to specify
the name of the secret in the pod spec
under the imagePullSecrets section 
as name : <Name-of-that-secret>
==
AUTOMATED WAY USING SERVICE ACCOUNT 
Another way is to create the image pull secret 
and then use it for a service account
and then specify the serviceaccount for the pod
Read more here
https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account
===
Also note that this secret has to be of the TYPE
kubernetes.io/dockercfg or kubernetes.io/dockerconfigjson
===
When kubelet needs to fetch registry credentials
dynamically, then use Kubelet credential provider
Approach mostly used by AWS and other cloud providers. 

Read more here : https://kubernetes.io/docs/tasks/administer-cluster/kubelet-credential-provider/
====
CREATION OF Secrets
To create secret imperatively, we can create three types of secrets
a) generic [ Created from file, directory or literal]
b) tls [Create a TLS secret from the given public/private key pair]
c) docker-registry : These are used to authenticate against docker private registries
from where the kubelet can pull images from 
===
Note that in secrets and configmap, when mounting as volume
The volumes sections differ a bit. 
****
In configmap, here is how the volumes look like

volumes:
- name: test-volume
  configMap: 
    name: my-cm 

However in case of secrets, the volumes are like this 
****
volumes:
- name: test-volume
  secret:
    secretName: my-secret
    items: "file-name-that-you-want-from-directory"
    path: "file-name-that-you-want-from-directory"
====

Exposing a deployment using service

kubectl expose deployment nginx --port 80 --target-port 80 --name mysvcname

====

Network Policies 

By default, all ingress and egress connections to pods
are allowed. 
One can attach a network policy 
that selects the pod and has either of the two 
policyTypes - egress or ingress. 

The only allowed egress connections in those
cases are those allowed by the egress list 
similarly for ingress list 

Note that these are additive

======

How to label [No -l required]

kubectl label pod/test  {bu=bu1,env=prod}
kubectl label deploy/nginx {owner=grv,env=prod}

Show labels
kubectl get pods --show-labels

Find resources with given labels
kubectl get pods -l bu=bu1;env=test
-l : serves as selector 

Remember to use "," to separate labels
and enclose them in paranthesis. 
====
Services are created using selector 
or without selector

kubectl expose pod/test --selector bu=bu1;app=test --port 80 --name mysvc

The above statement will create a service that exposes the 
pod test and selects using the selector. 

If you don't specify the selector argument 
then the service will use all the labels for the 
corresponding pod as selector 

However if you don't specify the 
selector while exposing deployment,
then the service by default choses 
app:<name-of-deploy> as selector 

You can also specify the --target-port for the
port on which the container is running. 


One can also describe a service without any selectors
in this case, you will have to manually 
create an endpointslice object that actually
represents the backend for the service. 

You link an EndpointSlice to a Service by setting the kubernetes.io/service-name label on that EndpointSlice.

This is typically used when you want to
create a service with backend outside of k8s cluster 

EndpointSlice are the replacements for endpoints
===

Headless service 

Sometimes you want to connect to pod directly 
instead of service IP 

eg. stateful nodes [ mysql with active/passive]
always want to connect to active node say 

So you will use a headless service
In that the clusterIP is specified as None
====
Each connection to the service is forwarded to one randomly 
selected backing pod. But what if the client needs to connect
to all of those pods? What if the backing pods themselves need
to each connect to all the other backing pods. Connecting 
through the service clearly isn’t the way to do this. What is?


For a client to connect to all pods, 
it needs to figure out the the IP of each individual pod.
One option is to have the client call the Kubernetes API server
and get the list of pods and their IP addresses through an API call,
but because you should always strive to keep your apps 
Kubernetes-agnostic, using the API server isn’t ideal.

Luckily, Kubernetes allows clients to discover pod IPs 
through DNS lookups. Usually, when you perform a DNS lookup 
for a service, the DNS server returns a single IP 
— the service’s cluster IP. 
But if you tell Kubernetes you don’t need a cluster IP
for your service (you do this by setting the clusterIP field to None
 in the service specification ), 
the DNS server will return the pod IPs instead of the single
service IP. Instead of returning a single DNS A record, 
the DNS server will return multiple A records for the service,
each pointing to the IP of an individual pod backing the service
at that moment. Clients can therefore do a simple DNS A record 
lookup and get the IPs of all the pods that are part of the service.
The client can then use that information to connect to one, 
many, or all of them.

Setting the clusterIP field in a service spec to None makes the service headless, as Kubernetes won’t assign it a cluster IP through which clients could connect to the pods backing it.

====
Default type of service is clusterIP.
The other type of service is NodePort
In this case, your service is available 
on a port [30000-32767] on the nodes of your K8s cluster

If you don't specify a NodePort in your config,
then K8s will randomly pick a port from 
30000-32767 range and allocate that to your service. 
Remember that service port and NodePort are two 
distinct things and so is TargetPort
TargetPort -> port on which container is listening.
Port -> port on which clusterIP service is listening
NodePort -> port on the node where your service is 
exposed and accessible externally to cluster
using your node-ip:node-port 

Note that when we specify a NodePort in our manifest
we have to make sure that this nodeport is within 
the 32000-32767 range and there is no port collisions
===
type = LoadBalancer
Creates a CLB in EKS.
===
Depending on the cloud provider, when you create a 
LoadBalancer service, you can also see a NodePort
service created. 
If you wanted to avoid creation of NodePort service, 
you can set "spec.allocateLoadBalancerNodePorts" to false 
in your manifest and that would not create a nodeport service. 

However this depends on the cloud provider implementation. 
=====
If you wanted to create a NLB in AWS when creating 
a service of type LoadBalancer,
you will have to add an annotation 

    metadata:
      name: my-service
      annotations:
        service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
======
ExternalName Type 
You can use a type of ExternalName
to map a service to a DNS name. 

This should be done in sceanrios where we are trying 
to reach out to external Services outside our cluster

apiVersion: v1
kind: Service
metadata:
  name: my-service
  namespace: prod
spec:
  type: ExternalName
  externalName: my.database.example.com
====
When looking up the host my-service.prod.svc.cluster.local, the cluster DNS Service 
returns a CNAME record with the value my.database.example.com.

One can use externalName service type to 
not only access services outside the cluster
such as RDS or DocumentDB cluster
but you can also use it 
to access any services outside the namespace.

===
More on Network Policy

By default, all egress and ingress communication to 
pods are allowed.
Pods in one namespace can reach to pods in other namespace

However, in many production scenarios, you 
won’t want that level of openness

So we can use NetworkPolicy to help restrict communication.

The way NetworkPolicy works is by creating a 
K8s object called NetworkPolicy[NP]

That NP has two things 
a) policyType - which can be egress or ingress
b) There would also be a selector. This will specify 
which pods or namespaces will this NP apply to

The overall structure of a network policy is 
apiversion 
metadata [contains name and any labels you would want to associate]
spec 
status 

Under the spec, we have the podSelector, which is a required
field.
We can use labels to select the pods to which the NP 
applies to.
You can also select all the pods in the current namespace 
by leaving the field in podSelector empty 












